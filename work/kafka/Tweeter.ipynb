{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesando tweets con Kafka\n",
    "\n",
    "El objetivo del ejercicio es procesar los datos de la API de streaming de Tweeter para obtener el tending topic de la última hora. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesar datos de la API de streaming de Tweeter\n",
    "\n",
    "Vamos a emular la api de streaming de Tweeter. Para ellos nos vamos a valer de un ficheo en el que se han escrito todos los mensajes que nos ha devuelto la api de tweeter durante una hora.\n",
    "\n",
    "Iremos insertando esos mensajes en Kafka para procesarlos posteriormente en real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kafka-python\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def insert_tweet(tweet_json):\n",
    "    json_producer = KafkaProducer(bootstrap_servers='localhost:9092',\n",
    "                                  value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "    json_producer.send('tweetapi', tweet_json)\n",
    "    json_producer.flush()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pprintpp import pprint as pp\n",
    "import sys\n",
    "\n",
    "tweets_data_path = '../data/tweets.json'\n",
    "\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "for line in tweets_file:\n",
    "    tweet_json = json.loads(line)\n",
    "    insert_tweet(tweet_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leemos los mensajes de la cola de Kafka según llegan y los prcesamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "hashtags = []\n",
    "\n",
    "consumer = KafkaConsumer('jsontest', \n",
    "                         bootstrap_servers = ['localhost:9092'],\n",
    "                         value_deserializer = lambda m: json.loads(m.decode('utf-8')),\n",
    "                         consumer_timeout_ms = 10000,\n",
    "                         auto_offset_reset = 'earliest', \n",
    "                         enable_auto_commit = True)\n",
    "for msg in consumer:\n",
    "    entities = msg.value['entities']\n",
    "    for entity in entities:\n",
    "        hashtags.append((entity.lower(), 1))\n",
    "\n",
    "columns = ['hashtag', 'count']\n",
    "df = pd.DataFrame(hashtags, columns = columns)\n",
    "df2 = df.groupby(['hashtag']).sum()\n",
    "sorted_values = df2.sort_values(by = ['count'], ascending = False)\n",
    "print(sorted_values.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
